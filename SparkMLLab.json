{"paragraphs":[{"title":"K-Means Model Creation using SparkML","text":"import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\n\nval modelPath = \"s3://<s3-bucket-name>/model/model1/\"\n\n// Load and parse the data\nval file = sc.textFile(\"s3://serverless-analytics/zombie-datalake/world_factbook/WorldFactBook.csv\")\nval dataWithHeader = file.map(s=>s.split('|')).\n    filter(v=> v.length>37 && v(2) != \"\" && v(4)!=\"\" && v(18) !=\"\").\n    map(r=>(r(2).toString + \",\" + r(4).toString + \",\"+ r(18).toString))\n\nval header = dataWithHeader.take(1)\nval data = dataWithHeader.filter(s=>(s!=header(0)))\nval parsedData = data.map(s => Vectors.dense(s.split(',').map(_.toDouble))).cache()\n\nval numClusters = 3\nval numIterations = 40\nval clusters = KMeans.train(parsedData, numClusters, numIterations)\n\n// Evaluate clustering by computing Within Set Sum of Squared Errors\nval WSSSE = clusters.computeCost(parsedData)\nprintln(\"Within Set Sum of Squared Errors = \" + WSSSE)\n\nclusters.save(sc, modelPath)","user":"anonymous","dateUpdated":"2017-11-09T05:33:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510202238171_-447467480","id":"20171109-043718_1230506665","dateCreated":"2017-11-09T04:37:18+0000","dateStarted":"2017-11-09T04:39:18+0000","dateFinished":"2017-11-09T04:40:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1385"},{"title":"Running prediction based on the K-Means model using Spark Streaming","text":"import org.apache.spark._\nimport org.apache.spark.streaming._\nimport sqlContext._\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.Duration\nimport org.apache.spark.streaming.Seconds\nimport org.apache.spark.sql.types.{StructType,StructField,StringType}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.Row\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.linalg.Vectors\n\nval conf = new SparkConf().setAppName(\"predict-app\")\nval ssc = new StreamingContext(sc, Seconds(60))\n\nval dataSetStream = ssc.textFileStream(\"s3n://<s3-bucket-name>/data2017/*/*/*/\")\nval sameModel = KMeansModel.load(sc, modelPath)\n\nprintln(\"Stream Running....\")\n\ndataSetStream.foreachRDD((rdd:RDD[String])=>{  \n    if(!rdd.partitions.isEmpty){\n        println(\"Running prediction....\")\n        \n        println(\"Data\")\n        rdd.collect().foreach(println)\n        \n        //check if there are no datasets to be processed in this batch\n        val dataSetRdd = rdd.map(s => Vectors.dense(s.split(',').map(_.toDouble)))\n        val prData = sameModel.predict(dataSetRdd)\n        \n        println(\"Result\")\n        prData.collect().foreach(l=>println(l))\n    }\n});\n\nssc.start()\nssc.awaitTerminationOrTimeout(10*60*1000)","user":"anonymous","dateUpdated":"2017-11-09T05:34:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510202358723_-1187169405","id":"20171109-043918_1945312790","dateCreated":"2017-11-09T04:39:18+0000","dateStarted":"2017-11-09T04:55:08+0000","dateFinished":"2017-11-09T05:00:15+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1386"},{"title":"Terminate Spark Streaming Job","text":"ssc.stop(stopSparkContext=false, stopGracefully=true)","user":"anonymous","dateUpdated":"2017-11-09T05:34:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510202506426_-146241384","id":"20171109-044146_520755412","dateCreated":"2017-11-09T04:41:46+0000","dateStarted":"2017-11-09T05:00:24+0000","dateFinished":"2017-11-09T05:02:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1387"},{"text":"","user":"anonymous","dateUpdated":"2017-11-09T05:31:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510202673074_1034597318","id":"20171109-044433_1853144112","dateCreated":"2017-11-09T04:44:33+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1388"}],"name":"SparkMLLab","id":"2CZCQVS61","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}